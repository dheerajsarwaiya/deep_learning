{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample picture and learn how to recognize it.\n",
    "dheeraj_image = face_recognition.load_image_file(\"dheeraj.jpg\")\n",
    "dheeraj_encoding = face_recognition.face_encodings(dheeraj_image)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays of known face encodings and their names\n",
    "known_face_encodings = [\n",
    "    dheeraj_encoding\n",
    "]\n",
    "known_face_names = [\n",
    "    \"Dheeraj\"    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    rgb_frame = frame[:, :, ::-1]\n",
    "\n",
    "    # Find all the faces and face enqcodings in the frame of video\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "    # Loop through each face in this frame of video\n",
    "    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "        # See if the face is a match for the known face(s)\n",
    "        matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "\n",
    "        name = \"Unknown\"\n",
    "\n",
    "        # If a match was found in known_face_encodings, just use the first one.\n",
    "        if True in matches:\n",
    "            first_match_index = matches.index(True)\n",
    "            name = known_face_names[first_match_index]\n",
    "\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "        # Draw a label with a name below the face\n",
    "        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"paper1.JPG\")\n",
    "ratio = image.shape[0] / 500.0\n",
    "orig = image.copy()\n",
    "image = imutils.resize(image, height = 500)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Edge Detection\n"
     ]
    }
   ],
   "source": [
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "edged = cv2.Canny(gray, 75, 200)\n",
    " \n",
    "# show the original image and the edge detected image\n",
    "print(\"STEP 1: Edge Detection\")\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.imshow(\"Edged\", edged)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the contours in the edged image, keeping only the\n",
    "# largest ones, and initialize the screen contour\n",
    "cnts = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = cnts[0] if imutils.is_cv2() else cnts[1]\n",
    "cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transform import four_point_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 2: Find contours of paper\n"
     ]
    }
   ],
   "source": [
    "# loop over the contours\n",
    "for c in cnts:\n",
    "    # approximate the contour\n",
    "    peri = cv2.arcLength(c, True)\n",
    "    approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    " \n",
    "    # if our approximated contour has four points, then we\n",
    "    # can assume that we have found our screen\n",
    "    if len(approx) == 4:\n",
    "        screenCnt = approx\n",
    "        break\n",
    "      \n",
    "# show the contour (outline) of the piece of paper\n",
    "print(\"STEP 2: Find contours of paper\")\n",
    "cv2.drawContours(image, [screenCnt], -1, (0, 255, 0), 2)\n",
    "cv2.imshow(\"Outline\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import threshold_adaptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dheeraj/work/env1/lib/python3.6/site-packages/skimage/filters/thresholding.py:221: skimage_deprecation: Function ``threshold_adaptive`` is deprecated and will be removed in version 0.15. Use ``threshold_local`` instead.\n",
      "  def threshold_adaptive(image, block_size, method='gaussian', offset=0,\n",
      "/home/dheeraj/work/env1/lib/python3.6/site-packages/skimage/filters/thresholding.py:223: UserWarning: The return value of `threshold_local` is a threshold image, while `threshold_adaptive` returned the *thresholded* image.\n",
      "  warn('The return value of `threshold_local` is a threshold image, while '\n"
     ]
    }
   ],
   "source": [
    "show=True\n",
    "if screenCnt.__len__() != 0:\n",
    "    if show:\n",
    "        cv2.drawContours(image, [screenCnt], -1, (0, 255, 0), 2)\n",
    "        cv2.imwrite('outlined.jpg', image)\n",
    "        cv2.imshow(\"Outline\", image)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    warped = four_point_transform(orig, screenCnt.reshape(4, 2) * ratio)\n",
    "else:\n",
    "    warped = orig\n",
    "\n",
    "warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "warped = threshold_adaptive(warped, 251, offset=10)\n",
    "warped = warped.astype(\"uint8\") * 255\n",
    "\n",
    "if show:\n",
    "    cv2.imshow(\"Original\", util.resize(orig, height=650))\n",
    "    cv2.imshow(\"Scanned\", util.resize(warped, height=650))\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "from pytesseract import image_to_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import Image\n",
    "except ImportError:\n",
    "    from PIL import Image\n",
    "import pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ae eee\n",
      "\n",
      "eet Restaurant\n",
      "Main, > jusiness Drive\n",
      "Suite 528\n",
      "paso Alto California 9430\n",
      "575-1628095\n",
      "\n",
      "Fri 04/07/2017 3:57 PM\n",
      "\n",
      "Order ID: #4a59c18f\n",
      "Order Number: 1\n",
      "\n",
      "ements swede em ke hee a\n",
      "\n",
      "Chocolate Chip Cooki 5.00\n",
      "\n",
      "Apple Pie 3.00\n",
      "Lava Cake 4.00\n",
      "Sub Total «USD$ 12.00\n",
      "Grand Total: oo $12.00\n",
      "\n",
      "Card; RAKKXXXXXXXO 14.16\n",
      "Tip — | | 2.16\n",
      "\n",
      "em mek em hie tk fb DS D6 ft hho BD 2D Oo b® 6.S Sb ABO Ah\n"
     ]
    }
   ],
   "source": [
    "cv2.imwrite('scan_res.jpg', warped)\n",
    "text = pytesseract.image_to_string(Image.open('scan_res.jpg'), config=\"config\")\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "»\n",
      "Ne\n",
      "a\n",
      "eet\\\\\n",
      "“Sa\n",
      "dine\n",
      "a\n",
      "sy\n",
      "Say\n",
      "\n",
      "Hist\n",
      "Yuval Noah\n",
      "\n",
      "a!\n",
      "\n",
      "*\n",
      "mt\n",
      "\n",
      "me A Brie!\n",
      "Yuval N\n",
      "\n",
      "Saf\n",
      "A!\n",
      "\n",
      "antes\n",
      "Yuval!\n",
      "m6 (6\n",
      "ap\n",
      "\n",
      "; ABr\n",
      "Histol\n",
      "of\n",
      "\n",
      "Sap!\n",
      "ABi\n",
      "\n",
      "nea\n",
      ".\n",
      "~\n",
      "Sat\n",
      "Sal\n",
      "Sa\n",
      "4\n",
      "54\n",
      "Say\n",
      "Sa\n",
      "24\n",
      "Sa\n",
      "Yuval\n",
      "BS yova\n",
      "-)\n",
      "\n",
      "B yoval\n",
      "\\\n"
     ]
    }
   ],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "padding = 100\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "    height, width, channels = rgb_frame.shape\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "    rgb_frame = frame[:, :, ::-1]\n",
    "\n",
    "    # Find all the faces and face enqcodings in the frame of video\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "    # Loop through each face in this frame of video\n",
    "    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "        # See if the face is a match for the known face(s)\n",
    "        matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "\n",
    "        name = \"Unknown\"\n",
    "\n",
    "        # If a match was found in known_face_encodings, just use the first one.\n",
    "        if True in matches:\n",
    "            first_match_index = matches.index(True)\n",
    "            name = known_face_names[first_match_index]\n",
    "        \n",
    "        \n",
    "        h=top-bottom\n",
    "        \n",
    "        if h>height:\n",
    "            h=width-bottom\n",
    "        \n",
    "#         roi = im[y1:y2, x1:x2]\n",
    "        crop_img = frame[bottom:, left:right-left]\n",
    "#         cv2.rectangle(frame, (left, bottom), (right, bottom+height), (0, 255, 0), cv2.FILLED)\n",
    "\n",
    "        text = pytesseract.image_to_string(crop_img, config=\"config\")\n",
    "        if text !=\"\":\n",
    "            print(text)\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "#         # Draw a label with a name below the face\n",
    "#         cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "#         font = cv2.FONT_HERSHEY_DUPLEX\n",
    "#         cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History of\n",
      "Auman}\n",
      "\n",
      "Ind\n",
      "Sapiens\n",
      "A Brief\n",
      "History of\n",
      "Humankind\n",
      "Sapiens\n",
      "A Brief\n",
      "History of\n",
      "Humankind\n",
      "*\n",
      "Sapiens\n",
      "\n",
      "A Brief\n",
      "History of\n",
      "Humankind\n",
      "*\n",
      "Sapiens\n",
      "\n",
      "A Brief\n",
      "\n",
      "History of\n",
      "Humankind\n",
      "\n",
      " \n",
      "\n",
      "hy\n",
      "\n",
      "Bh oi\n",
      "*\n",
      "Sapiens\n",
      "\n",
      "A Briet\n",
      "\n",
      "History of\n",
      "Humankind\n",
      "\n",
      " \n",
      "\n",
      "LF\n",
      "\n",
      "Pr |\n",
      "a\n",
      "Sapiens\n",
      "\n",
      "A Briel\n",
      "History of\n",
      "\n",
      "Humankind\n",
      "Yuval Noah Harart\n",
      "\n",
      ".\n",
      "Sapiens\n",
      "A Brief\n",
      "History of\n",
      "\n",
      "Humankind\n",
      "Yuval Noah Harart\n",
      "\n",
      "a\n",
      "Sapiens\n",
      "\n",
      "A Brief\n",
      "\n",
      "History of\n",
      "\n",
      "Humankind\n",
      "Yuval Noah Harari\n",
      "\n",
      ".\n",
      "Sapiens\n",
      "\n",
      "A Brief\n",
      "\n",
      "History of\n",
      "Humankind\n",
      "Yuval Noah Harari\n",
      "\n",
      "*\n",
      "Sapiens\n",
      "\n",
      "A Brief\n",
      "\n",
      "History of\n",
      "Humankind\n",
      "Yuval Noah Harari\n",
      "\n",
      "®\n",
      "Sapiens\n",
      "\n",
      "\\ Brief\n",
      "\n",
      "History of\n",
      "\n",
      "Humankind\n",
      "Yuval Noah Harari\n",
      "A Brief\n",
      "\n",
      "History of\n",
      "I tUmank yg\n",
      "Yuval Noah Harari\n",
      "\n",
      "*\n",
      "Sapiens\n",
      "A Brief\n",
      "History of\n",
      "\n",
      "Ind\n",
      "\n",
      "   \n",
      "\n",
      "Auman,\n",
      "Nivel Noah Harari\n",
      "\n",
      "+\n",
      "Sapiens\n",
      "\n",
      "A Brief\n",
      "History of\n",
      "\n",
      "   \n",
      "\n",
      "Humank ed\n",
      "Yuval Noah Harari\n",
      "\n",
      "+\n",
      "Sapiens\n",
      "\n",
      "A Brief\n",
      "History of\n",
      "\n",
      "Ikd\n",
      "\n",
      "Tuman\n"
     ]
    }
   ],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "#     # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "#     rgb_frame = frame[:, :, ::-1]\n",
    "    \n",
    "    ratio = frame.shape[0] / 500.0\n",
    "    orig = frame.copy()\n",
    "    frame = imutils.resize(frame, height = 500)\n",
    "     \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    edged = cv2.Canny(gray, 75, 200)\n",
    "    \n",
    "    cnts = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = cnts[0] if imutils.is_cv2() else cnts[1]\n",
    "    cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n",
    "\n",
    "    # loop over the contours\n",
    "    for c in cnts:\n",
    "        # approximate the contour\n",
    "        peri = cv2.arcLength(c, True)\n",
    "        approx = cv2.approxPolyDP(c, 0.02 * peri, True)\n",
    "        # if our approximated contour has four points, then we\n",
    "        # can assume that we have found our screen\n",
    "        if len(approx) == 4:\n",
    "            screenCnt = approx\n",
    "            break\n",
    "\n",
    "    cv2.drawContours(frame, [screenCnt], -1, (0, 255, 0), 2)\n",
    "    \n",
    "    text = pytesseract.image_to_string(frame, config=\"config\")\n",
    "    if text !=\"\":\n",
    "        print(text)\n",
    "\n",
    "\n",
    "    # Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Hit 'q' on the keyboard to quit!\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release handle to the webcam\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
